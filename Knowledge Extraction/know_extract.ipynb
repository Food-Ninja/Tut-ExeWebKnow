{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02029bf",
   "metadata": {},
   "source": [
    "# Extraction of Relevant Action and Object Knowledge from the Web\n",
    "\n",
    "- Definition of Action and Object Knowledge\n",
    "- Extraction of Action Knowledge:\n",
    "    - choosing a starting action (\"Cutting\")\n",
    "    - gathering synonyms and hyponyms from WordNet, VerbNet and Thesaurus\n",
    "    - filtering the gathered verbs using Recipe1M+ and WikiHow\n",
    "- Extraction of Object Knowledge:\n",
    "    - choosing an object group (\"Fruits & Vegetables\")\n",
    "    - gathering all fruits and vegetables from the FoodOn\n",
    "    - filtering the gathered fruits and vegetables using Recipe1M+\n",
    "    - gathering task-specific information using 3 different embeddings, 2 LLMs (ChatGPT & GPT-4) as well as 2 Recipe1M+ filtering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9222df",
   "metadata": {},
   "source": [
    "## Extraction of Action Knowledge\n",
    "bla bla bla"
   ]
  },
  {
   "cell_type": "code",
   "id": "111e9ade",
   "metadata": {},
   "source": [
    "# imports\n",
    "import pywsd.lesk as lesk\n",
    "import pandas as pd\n",
    "from nltk.corpus import verbnet, wordnet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a28ccfc3",
   "metadata": {},
   "source": [
    "### Extracting Synonyms and Hyponyms from WordNet & VerbNet"
   ]
  },
  {
   "cell_type": "code",
   "id": "3e0e16ae",
   "metadata": {},
   "source": [
    "# setting the target action and an exemplary sentence\n",
    "target_action = \"cut\"\n",
    "example_sent = \"Cut the apple into 2 halves\"\n",
    "\n",
    "# extracting the synset for the given target action\n",
    "synset = lesk.cosine_lesk(example_sent, target_action, pos='v')\n",
    "print(f\"Synset: {synset}\")\n",
    "synset = wordnet.synset('cut.v.01')\n",
    "\n",
    "# gathering all synonyms and hyponyms from WordNet\n",
    "verbs = synset.lemma_names()\n",
    "for h in synset.hyponyms():\n",
    "    verbs.extend(h.lemma_names())\n",
    "    \n",
    "\n",
    "# gathering synonyms from VerbNet\n",
    "key = str(synset.lemmas()[0].key()).replace(\"::\", \"\")\n",
    "print(f'VerbNet Key: {key}')\n",
    "class_name = verbnet.classids(wordnetid='key')\n",
    "if not class_name:\n",
    "    classes = verbnet.classids(lemma=target_action)\n",
    "    for c in classes:\n",
    "        front = c.split('-')[0]\n",
    "        if str(front) == target_action:\n",
    "            class_name = c\n",
    "            # print(f'ClassName={class_name}')\n",
    "vn_class = verbnet.fileids(class_name)[0]\n",
    "verbs.extend(verbnet.lemmas(vnclass=vn_class))\n",
    "verbs = set(verbs)\n",
    "print(f\"{len(verbs)} synonyms or hyponyms found\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7eae286",
   "metadata": {},
   "source": [
    "# pre-process the found synonyms and hyponyms\n",
    "filtered_verbs = {v for v in verbs if \"_\" not in v}\n",
    "filtered_verbs = sorted(set(filtered_verbs))\n",
    "\n",
    "print(f\"{len(filtered_verbs)} remaining words:\")\n",
    "for verb in filtered_verbs:\n",
    "    print(verb)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "00a1eb51",
   "metadata": {},
   "source": [
    "### Filtering the extracted verbs"
   ]
  },
  {
   "cell_type": "code",
   "id": "24f84702",
   "metadata": {},
   "source": [
    "v_occurrences = \"./verb_occurrences.csv\"\n",
    "\n",
    "# read and map the (extracted) occurrence data\n",
    "voc_dat = pd.read_csv(v_occurrences)\n",
    "voc_dat = voc_dat.astype({'Title':'int','Title Desc':'int','Method':'int','Step Headline':'int','Step Desc':'int','SUM':'int'})\n",
    "\n",
    "# remove all verbs with 0 occurrences\n",
    "most_used = voc_dat[(voc_dat['SUM'] > 0)]\n",
    "print(f\"{len(most_used)} verbs that occur at least 1x\")\n",
    "\n",
    "# remove all verbs with too few available sentences (Step Desc <= threshold)\n",
    "thresh = 50\n",
    "most_used = most_used[(most_used['Step Desc'] >= thresh)]\n",
    "print(f\"{len(most_used)} verbs that cover more than {thresh} step descriptions:\")\n",
    "print(most_used)\n",
    "\n",
    "## not fitting: DROP, RIP, SHAVE, TAP\n",
    "## missing but important: FILLET, HASH, HALVE, MINCE, QUARTER, SLIVER"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f460415",
   "metadata": {},
   "source": [
    "## Extraction of Object Knowledge\n",
    "bla bla bla"
   ]
  },
  {
   "cell_type": "code",
   "id": "a149dd3b",
   "metadata": {},
   "source": [
    "# imports\n",
    "from rdflib import Graph, Literal, Namespace, RDF, RDFS, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "#from typing import List\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import torch\n",
    "import torchtext"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47eab54d",
   "metadata": {},
   "source": [
    "### Extracting fruits and vegetables from the FoodOn"
   ]
  },
  {
   "cell_type": "code",
   "id": "a757b842",
   "metadata": {},
   "source": [
    "# load the ontology and set the namespace prefixes\n",
    "foodon_loc = \"./foodon.owl\"\n",
    "g = Graph()\n",
    "g.parse(foodon_loc)\n",
    "\n",
    "FOOD = Namespace(\"http://purl.obolibrary.org/obo/\")\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "\n",
    "# get the fruit data through the SPARQL query \n",
    "query = prepareQuery(   \n",
    "    \"\"\"\n",
    "    SELECT ?fruit_label (SAMPLE(?fruit_id) AS ?rndm_fruit_id) (SAMPLE(?def) AS ?rndm_def)\n",
    "    WHERE {\n",
    "        ?fruit_id rdfs:label ?label.\n",
    "        ?fruit_id rdfs:subClassOf+ food:PO_0009001.\n",
    "        OPTIONAL { ?fruit_id food:IAO_0000115 ?def. }\n",
    "\n",
    "        BIND (LCASE(STR(?label)) AS ?str_label).\n",
    "        BIND (STRBEFORE(?str_label, \"(\") AS ?fruit_label).\n",
    "        FILTER CONTAINS(?str_label, \"whole\").\n",
    "        FILTER NOT EXISTS { ?fruit_id rdfs:subClassOf* food:PO_0030104. }\n",
    "        FILTER (?fruit_id != food:FOODON_03304644).\n",
    "    }\n",
    "    GROUP BY ?fruit_label\n",
    "    ORDER BY ?fruit_label\n",
    "    \"\"\",\n",
    "    initNs={\"food\": FOOD, \"rdfs\": RDFS}\n",
    ")\n",
    "fruits = g.query(query)\n",
    "\n",
    "# get the vegetable data through the SPARQL query \n",
    "query = prepareQuery(\n",
    "    \"\"\"\n",
    "    SELECT ?veg_label (SAMPLE(?veg_id) AS ?rndm_veg_id) (SAMPLE(?def) AS ?rndm_def)\n",
    "    WHERE {\n",
    "        ?veg_id rdfs:label ?label.\n",
    "        ?veg_id rdfs:subClassOf+ food:FOODON_03302008.\n",
    "        OPTIONAL { ?veg_id food:IAO_0000115 ?def. }\n",
    "\n",
    "        BIND (LCASE(STR(?label)) AS ?str_label).\n",
    "        BIND (STRBEFORE(?str_label, \"(\") AS ?veg_label).\n",
    "        FILTER NOT EXISTS { ?veg_id rdfs:subClassOf* food:FOODON_03302007. }\n",
    "    }\n",
    "    GROUP BY ?veg_label\n",
    "    ORDER BY ?veg_label\n",
    "    \"\"\",\n",
    "    initNs={\"food\": FOOD, \"rdfs\": RDFS}\n",
    ")\n",
    "veggies = g.query(query)\n",
    "\n",
    "# convert query results into panda dataframes for further analysis\n",
    "fruit_list = [(str(row[0]), str(row[1]), str(row[2])) for row in fruits]\n",
    "veggie_list = [(str(row[0]), str(row[1]), str(row[2])) for row in veggies]\n",
    "\n",
    "fruit_df = pd.DataFrame(fruit_list, columns=[\"label\", \"rndm_id\", \"rndm_def\"])\n",
    "veggie_df = pd.DataFrame(veggie_list, columns=[\"label\", \"rndm_id\", \"rndm_def\"])\n",
    "combined_df = pd.concat([fruit_df, veggie_df], ignore_index=True)\n",
    "print(combined_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "af268b9e",
   "metadata": {},
   "source": [
    "### Filter the fruits and vegetables using WikiHow and Recipe1M+ data"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec013c41",
   "metadata": {},
   "source": [
    "f_occurrences = \"./fruit_occurrences.csv\"\n",
    "\n",
    "# read and map the (extracted) occurrence data\n",
    "foc_dat = pd.read_csv(f_occurrences)\n",
    "foc_dat = foc_dat.astype({'Recipes-Title':'int','Recipes-Title [%]':'float', 'Recipes-Steps':'int','Recipes-Steps [%]':'float',\n",
    "                'WikiHow-Title':'int','WikiHow-Title [%]':'float', 'WikiHow-TitleDescription':'int','WikiHow-TitleDescription [%]':'float',\n",
    "                'WikiHow-Method':'int','WikiHow-Method [%]':'float', 'WikiHow-StepHeadline':'int','WikiHow-StepHeadline [%]':'float',\n",
    "                'WikiHow-StepDescription':'int','WikiHow-StepDescription [%]':'float'})\n",
    "\n",
    "# remove all items with too few occurrences in any column (less than 1%)\n",
    "thresh = 0.01\n",
    "most_used = foc_dat[(foc_dat['Recipes-Title [%]'] >= thresh) | (foc_dat['Recipes-Steps [%]'] >= thresh) | (foc_dat['WikiHow-Title [%]'] >= thresh) | \n",
    "                  (foc_dat['WikiHow-TitleDescription [%]'] >= thresh) | (foc_dat['WikiHow-Method [%]'] >= thresh) | (foc_dat['WikiHow-StepHeadline [%]'] >= thresh) |\n",
    "                  (foc_dat['WikiHow-StepDescription [%]'] >= thresh)]\n",
    "print(most_used)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "95610aef",
   "metadata": {},
   "source": [
    "### Extract object properties using embeddings"
   ]
  },
  {
   "cell_type": "code",
   "id": "8749061f",
   "metadata": {},
   "source": [
    "# possible food parts and their connection\n",
    "parts = ['core', 'shell', 'peel', 'stem']\n",
    "foods = list(most_used['Name'])\n",
    "food_parts = {\n",
    "    'almond': ['shell'],\n",
    "    'apple': ['core', 'peel'],\n",
    "    'banana': ['peel'],\n",
    "    'cherry': ['core'],\n",
    "    'citron': ['peel'],\n",
    "    'coconut': ['shell'],\n",
    "    'cucumber': ['peel', 'stem'],\n",
    "    'kumquat': ['peel'],\n",
    "    'lemon': ['peel'],\n",
    "    'lime': ['peel'],\n",
    "    'olive': ['core'],\n",
    "    'orange': ['peel'],\n",
    "    'pepper': ['stem'],\n",
    "    'pineapple': ['core', 'peel'],\n",
    "    'pumpkin': ['core', 'peel'],\n",
    "    'squash': ['core', 'peel'],\n",
    "    'strawberry': ['stem'],\n",
    "    'tomato': ['peel', 'stem']\n",
    "}\n",
    "\n",
    "# define function for finding the key based on the given concept name\n",
    "def find_key(concept):\n",
    "    concept_is_synset = \"bn:\" in concept\n",
    "    keys = [key for key in nasari.index_to_key if concept in key.lower()]\n",
    "    for key in keys:\n",
    "        cut = key.split('__')[1].lower()\n",
    "        if (cut == concept and not concept_is_synset) or (concept_is_synset and concept in key.lower()):\n",
    "            return key\n",
    "    return concept\n",
    "\n",
    "# load ConceptNet Numberbatch & NASARI embeddings\n",
    "numberbatch = gensim.models.KeyedVectors.load_word2vec_format('./numberbatch-en.txt', binary=False)\n",
    "nasari = gensim.models.KeyedVectors.load_word2vec_format('./NASARI_embed_english.txt', binary=False)\n",
    "\n",
    "# cosine similarity between GloVe embeddings\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
    "\n",
    "for f in foods:\n",
    "    for p in parts:\n",
    "        sim = torch.cosine_similarity(glove[f].unsqueeze(0), glove[p].unsqueeze(0)).item()\n",
    "        if sim >= 0.5:\n",
    "            print(f'[GloVe] Similarity for {p} in {f}: {sim}')\n",
    "\n",
    "# cosine similarity between ConceptNet Numberbatch embeddings\n",
    "for f in foods:\n",
    "    for p in parts:\n",
    "        sim = numberbatch.similarity(f, p)\n",
    "        if sim >= 0.3:\n",
    "            print(f'[CN Numberbatch] Similarity for {p} in {f}: {sim}')\n",
    "            \n",
    "# cosine similarity between NASARI embeddings\n",
    "# Sadly, the BabelNet synsets for core (bn:04772260n) does not exist in the NASARI embeddings and \n",
    "# for 'shell' we need to look for the concrete synset (bn:00071005n) instead \n",
    "parts_nasari = parts = ['bn:00071005n', 'peel_(fruit)', 'plant_stem']\n",
    "for f in foods:\n",
    "    for p in parts_nasari:\n",
    "        f_key = find_key(f)\n",
    "        p_key = find_key(p)\n",
    "        if (f_key in nasari.index_to_key) and (p_key in nasari.index_to_key):\n",
    "            sim = nasari.similarity(f_key, p_key)\n",
    "            if sim >= 0.75:\n",
    "                print(f'[NASARI] Similarity for {p} and {f}: {sim}')            "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "987ff2ae",
   "metadata": {},
   "source": [
    "### Extract object properties using Recipe1M+ data"
   ]
  },
  {
   "cell_type": "code",
   "id": "16d9505a",
   "metadata": {},
   "source": "## ToDo",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
