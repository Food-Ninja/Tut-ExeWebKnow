{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02029bf",
   "metadata": {},
   "source": [
    "# Extraction of Relevant Action and Object Knowledge from the Web\n",
    "\n",
    "Before starting this tutorial, make sure the necessary packages are installed (see requirements.txt). Additionally, the following files need to be downloaded / extracted into this folder:\n",
    "\n",
    "- *foodon.owl* - FoodOn used for extracting information about fruits and vegetables (found [here](https://github.com/FoodOntology/foodon))\n",
    "- *numberbatch-en.txt* - ConceptNet Numberbatch embeddings for object property extraction (download *English-only V. 19.08* from [here](https://github.com/commonsense/conceptnet-numberbatch?tab=readme-ov-file#downloads))\n",
    "- *NASARI_embed_english.txt* - NASARI embeddings also used for object property extraction (download *English - Embed(Wiki)* from [here](http://lcl.uniroma1.it/nasari/#two))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9222df",
   "metadata": {},
   "source": [
    "## Extraction of Action Knowledge\n",
    "\n",
    "The extraction of knowledge about different and relevant actions consists of three main steps:\n",
    "\n",
    "1. Setting the central verb & providing an exemplary sentence (e.g. 'cut')\n",
    "2. Extracting synonyms and hyponyms from WordNet & VerbNet\n",
    "3. Filtering the extracted words on their relevance using a recipe and a WikiHow corpus"
   ]
  },
  {
   "cell_type": "code",
   "id": "111e9ade",
   "metadata": {},
   "source": [
    "# imports\n",
    "import pywsd.lesk as lesk\n",
    "import pandas as pd\n",
    "from nltk.corpus import verbnet, wordnet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a28ccfc3",
   "metadata": {},
   "source": [
    "### Extracting Synonyms and Hyponyms from WordNet & VerbNet"
   ]
  },
  {
   "cell_type": "code",
   "id": "3e0e16ae",
   "metadata": {},
   "source": [
    "# setting the target action and an exemplary sentence\n",
    "target_action = \"cut\"\n",
    "example_sent = \"Cut the apple into 2 halves\"\n",
    "\n",
    "# extracting the synset for the given target action\n",
    "synset = lesk.cosine_lesk(example_sent, target_action, pos='v')\n",
    "print(f\"Synset: {synset}\")\n",
    "synset = wordnet.synset('cut.v.01')\n",
    "\n",
    "# gathering all synonyms and hyponyms from WordNet\n",
    "verbs = synset.lemma_names()\n",
    "for h in synset.hyponyms():\n",
    "    verbs.extend(h.lemma_names())\n",
    "\n",
    "# gathering synonyms from VerbNet\n",
    "key = str(synset.lemmas()[0].key()).replace(\"::\", \"\")\n",
    "print(f'VerbNet Key: {key}')\n",
    "class_name = verbnet.classids(wordnetid='key')\n",
    "if not class_name:\n",
    "    classes = verbnet.classids(lemma=target_action)\n",
    "    for c in classes:\n",
    "        front = c.split('-')[0]\n",
    "        if str(front) == target_action:\n",
    "            class_name = c\n",
    "            break\n",
    "verbs.extend(verbnet.lemmas(class_name))\n",
    "\n",
    "# removing duplicates and printing results\n",
    "verbs = set(verbs)\n",
    "print(f\"{len(verbs)} synonyms or hyponyms found\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7eae286",
   "metadata": {},
   "source": [
    "# pre-process the found synonyms and hyponyms\n",
    "filtered_verbs = {v for v in verbs if \"_\" not in v}\n",
    "filtered_verbs = sorted(set(filtered_verbs))\n",
    "\n",
    "print(f\"{len(filtered_verbs)} remaining words:\")\n",
    "for verb in filtered_verbs:\n",
    "    print(verb)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "00a1eb51",
   "metadata": {},
   "source": [
    "### Filtering the extracted verbs"
   ]
  },
  {
   "cell_type": "code",
   "id": "24f84702",
   "metadata": {},
   "source": [
    "# read the (extracted) occurrence data\n",
    "v_occurrences = \"./verb_occurrences.csv\"\n",
    "voc_dat = pd.read_csv(v_occurrences)\n",
    "\n",
    "# remove all verbs with 0 occurrences\n",
    "most_used = voc_dat[(voc_dat['SUM'] > 0)]\n",
    "print(f\"{len(most_used)} verbs that occur at least once\")\n",
    "\n",
    "# remove all verbs with too few available sentences (Step Desc <= threshold)\n",
    "thresh = 50\n",
    "most_used = most_used[(most_used['Step Desc'] >= thresh)]\n",
    "print(f\"{len(most_used)} verbs that occur in more than {thresh} step descriptions:\")\n",
    "print(most_used['Verb'].to_string(index=False))\n",
    "\n",
    "## not fitting: \n",
    "# DROP, RIP, SHAVE, TAP\n",
    "## missing but important:\n",
    "# FILLET, HASH, HALVE, MINCE, QUARTER, SLIVER"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f460415",
   "metadata": {},
   "source": [
    "## Extraction of Object Knowledge\n",
    "\n",
    "The extraction of knowledge about different objects and their task-specific properties consists of four main steps:\n",
    "\n",
    "1. Choosing a group of relevant objects (e.g. 'Fruits & Vegetables')\n",
    "2. Gathering all available objects of that group from a suitable taxonomy or ontology (e.g. FoodOn)\n",
    "3. Filtering the extracted objects using fitting text corpora (e.g. Recipe1M+)\n",
    "4. Gathering information about task-specific properties using 3 different word embeddings (GloVe, NASARI & ConceptNet Numberbatch) and a recipe corpus"
   ]
  },
  {
   "cell_type": "code",
   "id": "a149dd3b",
   "metadata": {},
   "source": [
    "# imports\n",
    "from rdflib import Graph, Literal, Namespace, RDF, RDFS, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import torch\n",
    "import torchtext"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47eab54d",
   "metadata": {},
   "source": [
    "### Extracting fruits and vegetables from the FoodOn"
   ]
  },
  {
   "cell_type": "code",
   "id": "a757b842",
   "metadata": {},
   "source": [
    "# load the ontology and set the namespace prefixes\n",
    "foodon_loc = \"./foodon.owl\"\n",
    "g = Graph()\n",
    "g.parse(foodon_loc)\n",
    "\n",
    "FOOD = Namespace(\"http://purl.obolibrary.org/obo/\")\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "\n",
    "# get the fruit data through the SPARQL query \n",
    "query = prepareQuery(   \n",
    "    \"\"\"\n",
    "    SELECT ?fruit_label (SAMPLE(?fruit_id) AS ?rndm_fruit_id) (SAMPLE(?def) AS ?rndm_def)\n",
    "    WHERE {\n",
    "        ?fruit_id rdfs:label ?label.\n",
    "        ?fruit_id rdfs:subClassOf+ food:PO_0009001.\n",
    "        OPTIONAL { ?fruit_id food:IAO_0000115 ?def. }\n",
    "\n",
    "        BIND (LCASE(STR(?label)) AS ?str_label).\n",
    "        BIND (STRBEFORE(?str_label, \"(\") AS ?fruit_label).\n",
    "        FILTER CONTAINS(?str_label, \"whole\").\n",
    "        FILTER NOT EXISTS { ?fruit_id rdfs:subClassOf* food:PO_0030104. }\n",
    "        FILTER (?fruit_id != food:FOODON_03304644).\n",
    "    }\n",
    "    GROUP BY ?fruit_label\n",
    "    ORDER BY ?fruit_label\n",
    "    \"\"\",\n",
    "    initNs={\"food\": FOOD, \"rdfs\": RDFS}\n",
    ")\n",
    "fruits = g.query(query)\n",
    "\n",
    "# get the vegetable data through the SPARQL query \n",
    "query = prepareQuery(\n",
    "    \"\"\"\n",
    "    SELECT ?veg_label (SAMPLE(?veg_id) AS ?rndm_veg_id) (SAMPLE(?def) AS ?rndm_def)\n",
    "    WHERE {\n",
    "        ?veg_id rdfs:label ?label.\n",
    "        ?veg_id rdfs:subClassOf+ food:FOODON_03302008.\n",
    "        OPTIONAL { ?veg_id food:IAO_0000115 ?def. }\n",
    "\n",
    "        BIND (LCASE(STR(?label)) AS ?str_label).\n",
    "        BIND (STRBEFORE(?str_label, \"(\") AS ?veg_label).\n",
    "        FILTER NOT EXISTS { ?veg_id rdfs:subClassOf* food:FOODON_03302007. }\n",
    "    }\n",
    "    GROUP BY ?veg_label\n",
    "    ORDER BY ?veg_label\n",
    "    \"\"\",\n",
    "    initNs={\"food\": FOOD, \"rdfs\": RDFS}\n",
    ")\n",
    "veggies = g.query(query)\n",
    "\n",
    "# convert query results into panda dataframes for further analysis\n",
    "fruit_list = [(str(row[0]), str(row[1]), str(row[2])) for row in fruits]\n",
    "veggie_list = [(str(row[0]), str(row[1]), str(row[2])) for row in veggies]\n",
    "\n",
    "fruit_df = pd.DataFrame(fruit_list, columns=[\"label\", \"rndm_id\", \"rndm_def\"])\n",
    "veggie_df = pd.DataFrame(veggie_list, columns=[\"label\", \"rndm_id\", \"rndm_def\"])\n",
    "combined_df = pd.concat([fruit_df, veggie_df], ignore_index=True)\n",
    "print(combined_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "af268b9e",
   "metadata": {},
   "source": [
    "### Filter the fruits and vegetables using WikiHow and Recipe1M+ data"
   ]
  },
  {
   "cell_type": "code",
   "id": "ec013c41",
   "metadata": {},
   "source": [
    "f_occurrences = \"./fruit_occurrences.csv\"\n",
    "\n",
    "# read and map the (extracted) occurrence data\n",
    "foc_dat = pd.read_csv(f_occurrences)\n",
    "foc_dat = foc_dat.astype({'Recipes-Title':'int','Recipes-Title [%]':'float', 'Recipes-Steps':'int','Recipes-Steps [%]':'float',\n",
    "                'WikiHow-Title':'int','WikiHow-Title [%]':'float', 'WikiHow-TitleDescription':'int','WikiHow-TitleDescription [%]':'float',\n",
    "                'WikiHow-Method':'int','WikiHow-Method [%]':'float', 'WikiHow-StepHeadline':'int','WikiHow-StepHeadline [%]':'float',\n",
    "                'WikiHow-StepDescription':'int','WikiHow-StepDescription [%]':'float'})\n",
    "\n",
    "# remove all items with too few occurrences in any column (less than 1%)\n",
    "thresh = 0.01\n",
    "most_used = foc_dat[(foc_dat['Recipes-Title [%]'] >= thresh) | (foc_dat['Recipes-Steps [%]'] >= thresh) | (foc_dat['WikiHow-Title [%]'] >= thresh) | \n",
    "                  (foc_dat['WikiHow-TitleDescription [%]'] >= thresh) | (foc_dat['WikiHow-Method [%]'] >= thresh) | (foc_dat['WikiHow-StepHeadline [%]'] >= thresh) |\n",
    "                  (foc_dat['WikiHow-StepDescription [%]'] >= thresh)]\n",
    "print(most_used)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "95610aef",
   "metadata": {},
   "source": [
    "### Extract object properties using 3 different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd262933",
   "metadata": {},
   "source": [
    "# prepare the list of possible fruits and possible food parts\n",
    "parts = ['core', 'shell', 'peel', 'stem']\n",
    "foods = list(most_used['Name'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7fe0b19",
   "metadata": {},
   "source": [
    "# GloVe embeddings\n",
    "glove_sim = 0.5\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
    "\n",
    "for f in foods:\n",
    "    for p in parts:\n",
    "        sim = torch.cosine_similarity(glove[f].unsqueeze(0), glove[p].unsqueeze(0)).item()\n",
    "        if sim >= glove_sim:\n",
    "            print(f'[GloVe] {f} hasPart {p} (Similarity: {sim})')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8749061f",
   "metadata": {},
   "source": [
    "# ConceptNet Numberbatch embeddings\n",
    "numberbatch_sim = 0.3\n",
    "numberbatch = gensim.models.KeyedVectors.load_word2vec_format('./numberbatch-en.txt', binary=False)\n",
    "\n",
    "# cosine similarity between ConceptNet Numberbatch embeddings\n",
    "for f in foods:\n",
    "    for p in parts:\n",
    "        sim = numberbatch.similarity(f, p)\n",
    "        if sim >= numberbatch_sim:\n",
    "            print(f'[CN Numberbatch] {f} hasPart {p} (Similarity: {sim})')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50fa929c",
   "metadata": {},
   "source": [
    "# NASARI embeddings \n",
    "parts_nasari = ['bn:00071005n', 'peel_(fruit)', 'plant_stem']\n",
    "nasari_sim = 0.75\n",
    "nasari = gensim.models.KeyedVectors.load_word2vec_format('./NASARI_embed_english.txt', binary=False)\n",
    "               \n",
    "# define function for finding the key based on the given concept name\n",
    "def find_key(concept):\n",
    "    concept_is_synset = \"bn:\" in concept\n",
    "    keys = [key for key in nasari.index_to_key if concept in key.lower()]\n",
    "    for key in keys:\n",
    "        cut = key.split('__')[1].lower()\n",
    "        if (cut == concept and not concept_is_synset) or (concept_is_synset and concept in key.lower()):\n",
    "            return key\n",
    "    return concept\n",
    "    \n",
    "# cosine similarity between NASARI embeddings\n",
    "# Sadly, the BabelNet synsets for core (bn:04772260n) does not exist in the NASARI embeddings and \n",
    "# for 'shell' we need to look for the concrete synset (bn:00071005n) instead \n",
    "for f in foods:\n",
    "    for p in parts_nasari:\n",
    "        f_key = find_key(f)\n",
    "        p_key = find_key(p)\n",
    "        if (f_key in nasari.index_to_key) and (p_key in nasari.index_to_key):\n",
    "            sim = nasari.similarity(f_key, p_key)\n",
    "            if sim >= nasari_sim:\n",
    "                print(f'[NASARI] {f} hasPart {p} (Similarity: {sim})')         "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "987ff2ae",
   "metadata": {},
   "source": [
    "### Extract object properties using Recipe1M+ data"
   ]
  },
  {
   "cell_type": "code",
   "id": "846d18ff",
   "metadata": {},
   "source": [
    "# read the (extracted) occurrence data from Recipe1M+\n",
    "p_occurrences = \"./part_occurrences.csv\"\n",
    "poc_dat = pd.read_csv(p_occurrences)\n",
    "print(poc_dat)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "16d9505a",
   "metadata": {},
   "source": [
    "# set the thresholds & possible parts\n",
    "parts = ['core', 'shell', 'peel', 'stem']\n",
    "recipe_thresh = 0.01\n",
    "step_thresh = 0.01\n",
    "\n",
    "# print object-part-relations\n",
    "for idx, row in poc_dat.iterrows():\n",
    "    for p in parts:\n",
    "        rec_ratio = row[f'{p}_R'] / row['recipes']\n",
    "        step_ratio = row[f'{p}_S'] / row['steps']\n",
    "        if rec_ratio >= recipe_thresh and step_ratio >= step_thresh:\n",
    "            fruit = row['food']\n",
    "            rec_perc = '{:.2f}'.format(rec_ratio*100)\n",
    "            step_perc = '{:.2f}'.format(step_ratio*100)\n",
    "            print(f'{fruit} hasPart {p} ({rec_perc}% of Recipes & {step_perc}% of Steps)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a5bc1f2",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
