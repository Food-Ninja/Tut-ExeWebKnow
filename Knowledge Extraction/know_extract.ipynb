{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02029bf",
   "metadata": {},
   "source": [
    "# Extraction of Relevant Action and Object Knowledge from the Web\n",
    "\n",
    "Before starting this tutorial, make sure the necessary packages are installed (see requirements.txt). Additionally, the following files need to be downloaded / extracted into this folder:\n",
    "\n",
    "- *foodon.owl* - FoodOn used for extracting information about fruits and vegetables (found [here](https://github.com/FoodOntology/foodon))\n",
    "- *numberbatch-en.txt* - ConceptNet Numberbatch embeddings for object property extraction (download *English-only V. 19.08* from [here](https://github.com/commonsense/conceptnet-numberbatch?tab=readme-ov-file#downloads))\n",
    "- *NASARI_embed_english.txt* - NASARI embeddings also used for object property extraction (download *English - Embed(Wiki)* from [here](http://lcl.uniroma1.it/nasari/#two))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9222df",
   "metadata": {},
   "source": [
    "## Extraction of Action Knowledge\n",
    "\n",
    "The extraction of knowledge about different and relevant actions consists of three main steps:\n",
    "\n",
    "1. Setting the central verb & providing an exemplary sentence (e.g. 'cut')\n",
    "2. Extracting synonyms and hyponyms from WordNet & VerbNet\n",
    "3. Filtering the extracted words on their relevance using a recipe and a WikiHow corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e9ade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T12:44:23.298801Z",
     "start_time": "2024-05-13T12:44:23.290357Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "\n",
    "# download wordnet & verbnet corpus\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('verbnet')\n",
    "from nltk.corpus import verbnet, wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ccfc3",
   "metadata": {},
   "source": [
    "### Extracting Synonyms and Hyponyms from WordNet & VerbNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e16ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T12:53:27.876932Z",
     "start_time": "2024-05-13T12:44:30.582012Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting the target action and an exemplary sentence\n",
    "target_action = \"cut\"\n",
    "verbs = []\n",
    "\n",
    "# iterating over all WordNet synsets containing the verb and ...\n",
    "synsets = wordnet.synsets(target_action, pos=wordnet.VERB)\n",
    "print(f\"{len(synsets)} synsets found for '{target_action}'\")\n",
    "for syn in synsets:\n",
    "    # ... gathering all synonyms & direct hyponyms\n",
    "    verbs.extend(syn.lemma_names())\n",
    "    for h in syn.hyponyms():\n",
    "        verbs.extend(h.lemma_names())\n",
    "\n",
    "    # ... getting the associated VerbNet class\n",
    "    key = str(syn.lemmas()[0].key()).replace(\"::\", \"\")\n",
    "    vn_classes = verbnet.classids(wordnetid=key)\n",
    "    for vn_class in vn_classes:\n",
    "        verbs.extend(verbnet.lemmas(vn_class))\n",
    "\n",
    "# removing duplicates and printing results\n",
    "verbs = set(verbs)\n",
    "print(f\"{len(verbs)} synonyms or hyponyms found for '{target_action}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eae286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process the found synonyms and hyponyms\n",
    "filtered_verbs = {v.split('_')[0] for v in verbs}\n",
    "filtered_verbs = sorted(set(filtered_verbs))\n",
    "\n",
    "print(f\"{len(filtered_verbs)} remaining words:\")\n",
    "for verb in filtered_verbs:\n",
    "    print(verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1eb51",
   "metadata": {},
   "source": [
    "### Filtering the extracted verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f84702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the (extracted) occurrence data\n",
    "v_occurrences = \"./verb_occurrences.csv\"\n",
    "voc_dat = pd.read_csv(v_occurrences)\n",
    "\n",
    "# remove all verbs with 0 occurrences\n",
    "most_used = voc_dat[(voc_dat['SUM'] > 0)]\n",
    "print(f\"{len(most_used)} verbs that occur at least once\")\n",
    "\n",
    "# remove all verbs with too few available sentences (Step Desc <= threshold)\n",
    "thresh = 100\n",
    "most_used = most_used[(most_used['Step Desc'] >= thresh)]\n",
    "print(f\"{len(most_used)} verbs that occur in more than {thresh} step descriptions:\")\n",
    "print(most_used['Verb'].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f460415",
   "metadata": {},
   "source": [
    "## Extraction of Object Knowledge\n",
    "\n",
    "The extraction of knowledge about different objects and their task-specific properties consists of four main steps:\n",
    "\n",
    "1. Choosing a group of relevant objects (e.g. 'Fruits & Vegetables')\n",
    "2. Gathering all available objects of that group from a suitable taxonomy or ontology (e.g. FoodOn)\n",
    "3. Filtering the extracted objects using fitting text corpora (e.g. Recipe1M+)\n",
    "4. Gathering information about task-specific properties using 3 different word embeddings (GloVe, NASARI & ConceptNet Numberbatch) and a recipe corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a149dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from rdflib import Graph, Literal, Namespace, RDF, RDFS, URIRef\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eab54d",
   "metadata": {},
   "source": [
    "### Extracting fruits and vegetables from the FoodOn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a757b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ontology and set the namespace prefixes\n",
    "foodon_loc = \"./foodon.owl\"\n",
    "g = Graph()\n",
    "g.parse(foodon_loc)\n",
    "\n",
    "FOOD = Namespace(\"http://purl.obolibrary.org/obo/\")\n",
    "RDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\n",
    "\n",
    "# get the fruit data through the SPARQL query \n",
    "query = prepareQuery(   \n",
    "    \"\"\"\n",
    "    SELECT ?fruit_label (SAMPLE(?fruit_id) AS ?rndm_fruit_id) (SAMPLE(?def) AS ?rndm_def)\n",
    "    WHERE {\n",
    "        ?fruit_id rdfs:label ?label.\n",
    "        ?fruit_id rdfs:subClassOf+ food:PO_0009001.\n",
    "        OPTIONAL { ?fruit_id food:IAO_0000115 ?def. }\n",
    "\n",
    "        BIND (LCASE(STR(?label)) AS ?str_label).\n",
    "        BIND (STRBEFORE(?str_label, \"(\") AS ?fruit_label).\n",
    "        FILTER CONTAINS(?str_label, \"whole\").\n",
    "        FILTER NOT EXISTS { ?fruit_id rdfs:subClassOf* food:PO_0030104. }\n",
    "        FILTER (?fruit_id != food:FOODON_03304644).\n",
    "    }\n",
    "    GROUP BY ?fruit_label\n",
    "    ORDER BY ?fruit_label\n",
    "    \"\"\",\n",
    "    initNs={\"food\": FOOD, \"rdfs\": RDFS}\n",
    ")\n",
    "fruits = g.query(query)\n",
    "\n",
    "# get the vegetable data through the SPARQL query \n",
    "query = prepareQuery(\n",
    "    \"\"\"\n",
    "    SELECT ?veg_label (SAMPLE(?veg_id) AS ?rndm_veg_id) (SAMPLE(?def) AS ?rndm_def)\n",
    "    WHERE {\n",
    "        ?veg_id rdfs:label ?label.\n",
    "        ?veg_id rdfs:subClassOf+ food:FOODON_03302008.\n",
    "        OPTIONAL { ?veg_id food:IAO_0000115 ?def. }\n",
    "\n",
    "        BIND (LCASE(STR(?label)) AS ?str_label).\n",
    "        BIND (STRBEFORE(?str_label, \"(\") AS ?veg_label).\n",
    "        FILTER NOT EXISTS { ?veg_id rdfs:subClassOf* food:FOODON_03302007. }\n",
    "    }\n",
    "    GROUP BY ?veg_label\n",
    "    ORDER BY ?veg_label\n",
    "    \"\"\",\n",
    "    initNs={\"food\": FOOD, \"rdfs\": RDFS}\n",
    ")\n",
    "veggies = g.query(query)\n",
    "\n",
    "# convert query results into panda dataframes for further analysis\n",
    "fruit_list = [(str(row[0]), str(row[1]), str(row[2])) for row in fruits]\n",
    "veggie_list = [(str(row[0]), str(row[1]), str(row[2])) for row in veggies]\n",
    "\n",
    "fruit_df = pd.DataFrame(fruit_list, columns=[\"label\", \"rndm_id\", \"rndm_def\"])\n",
    "veggie_df = pd.DataFrame(veggie_list, columns=[\"label\", \"rndm_id\", \"rndm_def\"])\n",
    "combined_df = pd.concat([fruit_df, veggie_df], ignore_index=True)\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af268b9e",
   "metadata": {},
   "source": [
    "### Filter the fruits and vegetables using WikiHow and Recipe1M+ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec013c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_occurrences = \"./fruit_occurrences.csv\"\n",
    "\n",
    "# read and map the (extracted) occurrence data\n",
    "foc_dat = pd.read_csv(f_occurrences)\n",
    "foc_dat = foc_dat.astype({'Recipes-Title':'int','Recipes-Title [%]':'float', 'Recipes-Steps':'int','Recipes-Steps [%]':'float',\n",
    "                'WikiHow-Title':'int','WikiHow-Title [%]':'float', 'WikiHow-TitleDescription':'int','WikiHow-TitleDescription [%]':'float',\n",
    "                'WikiHow-Method':'int','WikiHow-Method [%]':'float', 'WikiHow-StepHeadline':'int','WikiHow-StepHeadline [%]':'float',\n",
    "                'WikiHow-StepDescription':'int','WikiHow-StepDescription [%]':'float'})\n",
    "\n",
    "# remove all items with too few occurrences in any column (less than 1%)\n",
    "thresh = 0.01\n",
    "most_used = foc_dat[(foc_dat['Recipes-Title [%]'] >= thresh) | (foc_dat['Recipes-Steps [%]'] >= thresh) | (foc_dat['WikiHow-Title [%]'] >= thresh) | \n",
    "                  (foc_dat['WikiHow-TitleDescription [%]'] >= thresh) | (foc_dat['WikiHow-Method [%]'] >= thresh) | (foc_dat['WikiHow-StepHeadline [%]'] >= thresh) |\n",
    "                  (foc_dat['WikiHow-StepDescription [%]'] >= thresh)]\n",
    "print(most_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95610aef",
   "metadata": {},
   "source": [
    "### Extract object properties using 3 different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd262933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the list of possible fruits and possible food parts\n",
    "parts = ['core', 'shell', 'peel', 'stem']\n",
    "foods = list(most_used['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe0b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe embeddings\n",
    "glove_sim = 0.5\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
    "\n",
    "for f in foods:\n",
    "    for p in parts:\n",
    "        sim = torch.cosine_similarity(glove[f].unsqueeze(0), glove[p].unsqueeze(0)).item()\n",
    "        if sim >= glove_sim:\n",
    "            print(f'[GloVe] {f} hasPart {p} (Similarity: {sim})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConceptNet Numberbatch embeddings\n",
    "numberbatch_sim = 0.3\n",
    "numberbatch = gensim.models.KeyedVectors.load_word2vec_format('./numberbatch-en.txt', binary=False)\n",
    "\n",
    "# cosine similarity between ConceptNet Numberbatch embeddings\n",
    "for f in foods:\n",
    "    for p in parts:\n",
    "        sim = numberbatch.similarity(f, p)\n",
    "        if sim >= numberbatch_sim:\n",
    "            print(f'[CN Numberbatch] {f} hasPart {p} (Similarity: {sim})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASARI embeddings \n",
    "parts_nasari = ['bn:00071005n', 'peel_(fruit)', 'plant_stem']\n",
    "nasari_sim = 0.75\n",
    "nasari = gensim.models.KeyedVectors.load_word2vec_format('./NASARI_embed_english.txt', binary=False)\n",
    "               \n",
    "# define function for finding the key based on the given concept name\n",
    "def find_key(concept):\n",
    "    concept_is_synset = \"bn:\" in concept\n",
    "    keys = [key for key in nasari.index_to_key if concept in key.lower()]\n",
    "    for key in keys:\n",
    "        cut = key.split('__')[1].lower()\n",
    "        if (cut == concept and not concept_is_synset) or (concept_is_synset and concept in key.lower()):\n",
    "            return key\n",
    "    return concept\n",
    "    \n",
    "# cosine similarity between NASARI embeddings\n",
    "# Sadly, the BabelNet synsets for core (bn:04772260n) does not exist in the NASARI embeddings and \n",
    "# for 'shell' we need to look for the concrete synset (bn:00071005n) instead \n",
    "for f in foods:\n",
    "    for p in parts_nasari:\n",
    "        f_key = find_key(f)\n",
    "        p_key = find_key(p)\n",
    "        if (f_key in nasari.index_to_key) and (p_key in nasari.index_to_key):\n",
    "            sim = nasari.similarity(f_key, p_key)\n",
    "            if sim >= nasari_sim:\n",
    "                print(f'[NASARI] {f} hasPart {p} (Similarity: {sim})')         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987ff2ae",
   "metadata": {},
   "source": [
    "### Extract object properties using Recipe1M+ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the (extracted) occurrence data from Recipe1M+\n",
    "p_occurrences = \"./part_occurrences.csv\"\n",
    "poc_dat = pd.read_csv(p_occurrences)\n",
    "print(poc_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the thresholds & possible parts\n",
    "parts = ['core', 'shell', 'peel', 'stem']\n",
    "recipe_thresh = 0.01\n",
    "step_thresh = 0.01\n",
    "\n",
    "# print object-part-relations\n",
    "for idx, row in poc_dat.iterrows():\n",
    "    for p in parts:\n",
    "        rec_ratio = row[f'{p}_R'] / row['recipes']\n",
    "        step_ratio = row[f'{p}_S'] / row['steps']\n",
    "        if rec_ratio >= recipe_thresh and step_ratio >= step_thresh:\n",
    "            fruit = row['food']\n",
    "            rec_perc = '{:.2f}'.format(rec_ratio*100)\n",
    "            step_perc = '{:.2f}'.format(step_ratio*100)\n",
    "            print(f'{fruit} hasPart {p} ({rec_perc}% of Recipes & {step_perc}% of Steps)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5bc1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
